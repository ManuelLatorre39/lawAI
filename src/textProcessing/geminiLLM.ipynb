{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemini_client():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"models/embedding-001\"):\n",
    "    \"\"\"Get embedding for a single piece of text\"\"\"\n",
    "    result = genai.embed_content(\n",
    "        model=model,\n",
    "        content=text,\n",
    "        task_type=\"retrieval_document\"  # or \"retrieval_query\", \"classification\", etc.\n",
    "    )\n",
    "    return result['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675edfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_context(path_schema):\n",
    "    #read schema\n",
    "    with open(path_schema, 'r', encoding='utf-8') as f:\n",
    "        context_data = json.load(f)\n",
    "        context_str = json.dumps(context_data) #convert to string well formatted\n",
    "        context_str = \"**JSON SCHEMA TO FOLLOW:**\\n\" + context_str\n",
    "    return context_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(texto):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.split_text(texto)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5de0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunks():\n",
    "    #leemos schema\n",
    "    with open('../../JSONS/baseSchema.json', 'r', encoding='utf-8') as f:\n",
    "        json_schema = json.load(f)\n",
    "    \n",
    "    #leemos prompt para el LLM (inicial)\n",
    "    with open('jsonFillPrompt.txt', 'r', encoding='utf-8') as f:\n",
    "        initial_instructions = f.read()\n",
    "    \n",
    "    #leemos prompt de refinamiento\n",
    "    with open('jsonRefinePrompt.txt', 'r', encoding='utf-8') as f:\n",
    "        refine_instructions = f.read()\n",
    "\n",
    "    #leemos documento legal\n",
    "    reader = PdfReader('../../Documentos/CODIGO PENAL DE LA NACION ARGENTINA.pdf')\n",
    "    legal_doc = \"\"\n",
    "    for page in reader.pages:\n",
    "        legal_doc += page.extract_text() + \"\\n\"\n",
    "\n",
    "    # Dividir en chunks\n",
    "    legal_doc_chunks = chunk_text(legal_doc)\n",
    "    print(f\"Total de chunks: {len(legal_doc_chunks)}\")\n",
    "    \n",
    "    # Limitar a 8 chunks\n",
    "    legal_doc_chunks = legal_doc_chunks[:2]\n",
    "    print(f\"Procesando solo los primeros {len(legal_doc_chunks)} chunks\")\n",
    "\n",
    "    # PASO 1: Crear JSON inicial con el primer chunk + schema completo\n",
    "    first_chunk_prompt = (\n",
    "        initial_instructions +\n",
    "        \"\\n\\n**JSON SCHEMA:**\\n\" +\n",
    "        json.dumps(json_schema, indent=2) +\n",
    "        \"\\n\\n**DOCUMENT:**\\n\" +\n",
    "        legal_doc_chunks[0]\n",
    "    )\n",
    "    \n",
    "    refined_json_dict = {}\n",
    "    \n",
    "    print(\"Procesando chunk 1...\")\n",
    "    model = GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "    response = model.generate_content(first_chunk_prompt)\n",
    "    # Parsear respuesta inicial\n",
    "    try:\n",
    "        refined_json_dict = json.loads(response.text)\n",
    "    except json.JSONDecodeError:\n",
    "        # If model added extra text, extract JSON portion\n",
    "        import re\n",
    "        match = re.search(r'\\{.*\\}', response.text, re.DOTALL)\n",
    "        if match:\n",
    "            refined_json_dict = json.loads(match.group(0))\n",
    "        else:\n",
    "            raise ValueError(\"First chunk did not return valid JSON\")\n",
    "    \n",
    "    # print(\"JSON inicial creado: \", response.text)\n",
    "    # PASO 2: Refinar con chunks restantes (SIN enviar el schema completo)\n",
    "    for i, chunk in enumerate(legal_doc_chunks[1:], start=2):\n",
    "        print(f\"Procesando chunk {i}/{len(legal_doc_chunks)}...\")\n",
    "        \n",
    "        # Prompt simplificado: instrucciones + JSON actual + nuevo chunk\n",
    "        refine_prompt = (\n",
    "            refine_instructions +\n",
    "            \"\\n\\n**EXISTING JSON:**\\n\" +\n",
    "            json.dumps(refined_json_dict, ensure_ascii=False) +\n",
    "            \"\\n\\n**NEW CHUNK:**\\n\" +\n",
    "            chunk\n",
    "        )\n",
    "        \n",
    "        response = model.generate_content(refine_prompt)\n",
    "        # Actualizar JSON refinado\n",
    "        # refined_json = response.text\n",
    "        \n",
    "        # Parse JSON safely\n",
    "        try:\n",
    "            refined_json_dict = json.loads(response.text)\n",
    "        except json.JSONDecodeError:\n",
    "            match = re.search(r'\\{.*\\}', response.text, re.DOTALL)\n",
    "            if match:\n",
    "                refined_json_dict = json.loads(match.group(0))\n",
    "            else:\n",
    "                print(f\"Warning: Chunk {i} returned invalid JSON, skipping update\")\n",
    "                \n",
    "        # Guardar resultado final\n",
    "        print(\"Guardando resultado...\")\n",
    "        with open('./processedDocs/response.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(refined_json_dict, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "        return refined_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81061bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "index_name = \"lawai_legal_docs\"\n",
    "\n",
    "es.indices.delete(index=index_name, ignore=[400, 404])\n",
    "\n",
    "with open(\"../elasticSearch/lawai_mapping.json\") as f:\n",
    "    mapping = json.load(f)[\"mapping\"]\n",
    "\n",
    "# es.indices.create(index=\"lawai\", mappings=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f039bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerativeModel\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # División de texto\n",
    "from elasticsearch import Elasticsearch\n",
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c25d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_ollama(text: str, model=\"nomic-embed-text\"):\n",
    "    response = ollama.embeddings(model=model, prompt=text)\n",
    "    return response[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "50508f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index already exists.\n",
      "Document indexed successfully!\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "if __name__ == \"__main__\":\n",
    "    get_gemini_client()\n",
    "    \n",
    "    # refined_json_dict = process_chunks()\n",
    "    with open('./processedDocs/response.json') as f:\n",
    "        refined_json_dict = json.load(f)  \n",
    "       \n",
    "    # Elastic Search: define index\n",
    "    with open('../elasticSearch/lawai_mapping.json') as f:\n",
    "        lawai_mapping = json.load(f)  \n",
    "    \n",
    "    es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "    index_name = \"lawai_legal_docs\"\n",
    "    \n",
    "    if not es.indices.exists(index=index_name):\n",
    "        es.indices.create(index=index_name, body=lawai_mapping[\"mapping\"])\n",
    "        print(f\"Index '{index_name}' created successfully!\")\n",
    "    else:\n",
    "        print(\"Index already exists.\")\n",
    "    \n",
    "\n",
    "    # refined_json_dict = json.loads(refined_json)\n",
    "    \n",
    "    # generate embeddings for the full text\n",
    "    full_text = refined_json_dict[\"content\"][\"full_text\"]\n",
    "    # embedding = get_embedding(full_text)\n",
    "    embedding = ollama.embeddings(model=\"nomic-embed-text\", prompt=full_text)[\"embedding\"]\n",
    "\n",
    "    # 3. store in JSON\n",
    "    refined_json_dict.setdefault(\"analysis\", {})\n",
    "    refined_json_dict[\"analysis\"][\"embeddings\"] = embedding\n",
    "    \n",
    "    # Index (insert) the document\n",
    "    es.index(\n",
    "        index=index_name, \n",
    "        id=refined_json_dict[\"document_id\"], \n",
    "        document=refined_json_dict\n",
    "    )\n",
    "    print(\"Document indexed successfully!\")\n",
    "    # print(\"¡Proceso completado!: \\n\"+ refined_json_dict)\n",
    "\n",
    "    # Query\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"metadata.title\": \"Constitución\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = es.search(index=index_name, body=query)\n",
    "    for hit in results[\"hits\"][\"hits\"]:\n",
    "        print(hit[\"_source\"][\"metadata\"][\"title\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
