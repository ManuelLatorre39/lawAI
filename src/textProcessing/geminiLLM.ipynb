{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f039bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerativeModel\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # División de texto\n",
    "from elasticsearch import Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "597f2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemini_client():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4fa77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"models/embedding-001\"):\n",
    "    \"\"\"Get embedding for a single piece of text\"\"\"\n",
    "    result = genai.embed_content(\n",
    "        model=model,\n",
    "        content=text,\n",
    "        task_type=\"retrieval_document\"  # or \"retrieval_query\", \"classification\", etc.\n",
    "    )\n",
    "    return result['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "675edfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_context(path_schema):\n",
    "    #read schema\n",
    "    with open(path_schema, 'r', encoding='utf-8') as f:\n",
    "        context_data = json.load(f)\n",
    "        context_str = json.dumps(context_data) #convert to string well formatted\n",
    "        context_str = \"**JSON SCHEMA TO FOLLOW:**\\n\" + context_str\n",
    "    return context_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6db5012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(texto):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.split_text(texto)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50508f64",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.generativeai' has no attribute 'TextEmbeddingModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      3\u001b[39m     get_gemini_client()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     embedding_model = \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextEmbeddingModel\u001b[49m()\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m#leemos schema\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m../../JSONS/baseSchema.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mAttributeError\u001b[39m: module 'google.generativeai' has no attribute 'TextEmbeddingModel'"
     ]
    }
   ],
   "source": [
    "#main\n",
    "if __name__ == \"__main__\":\n",
    "    get_gemini_client()\n",
    "    \n",
    "    #leemos schema\n",
    "    with open('../../JSONS/baseSchema.json', 'r', encoding='utf-8') as f:\n",
    "        json_schema = json.load(f)\n",
    "    \n",
    "    #leemos prompt para el LLM (inicial)\n",
    "    with open('jsonFillPrompt.txt', 'r', encoding='utf-8') as f:\n",
    "        initial_instructions = f.read()\n",
    "    \n",
    "    #leemos prompt de refinamiento\n",
    "    with open('jsonRefinePrompt.txt', 'r', encoding='utf-8') as f:\n",
    "        refine_instructions = f.read()\n",
    "\n",
    "    #leemos documento legal\n",
    "    reader = PdfReader('../../Documentos/CODIGO PENAL DE LA NACION ARGENTINA.pdf')\n",
    "    legal_doc = \"\"\n",
    "    for page in reader.pages:\n",
    "        legal_doc += page.extract_text() + \"\\n\"\n",
    "\n",
    "    # Dividir en chunks\n",
    "    legal_doc_chunks = chunk_text(legal_doc)\n",
    "    print(f\"Total de chunks: {len(legal_doc_chunks)}\")\n",
    "    \n",
    "    # Limitar a 8 chunks\n",
    "    legal_doc_chunks = legal_doc_chunks[:2]\n",
    "    print(f\"Procesando solo los primeros {len(legal_doc_chunks)} chunks\")\n",
    "\n",
    "    # PASO 1: Crear JSON inicial con el primer chunk + schema completo\n",
    "    first_chunk_prompt = (\n",
    "        initial_instructions +\n",
    "        \"\\n\\n**JSON SCHEMA:**\\n\" +\n",
    "        json.dumps(json_schema, indent=2) +\n",
    "        \"\\n\\n**DOCUMENT:**\\n\" +\n",
    "        legal_doc_chunks[0]\n",
    "    )\n",
    "    \n",
    "    refined_json_dict = {}\n",
    "    \n",
    "    print(\"Procesando chunk 1...\")\n",
    "    model = GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "    response = model.generate_content(first_chunk_prompt)\n",
    "    # Parsear respuesta inicial\n",
    "    try:\n",
    "        refined_json_dict = json.loads(response.text)\n",
    "    except json.JSONDecodeError:\n",
    "        # If model added extra text, extract JSON portion\n",
    "        import re\n",
    "        match = re.search(r'\\{.*\\}', response.text, re.DOTALL)\n",
    "        if match:\n",
    "            refined_json_dict = json.loads(match.group(0))\n",
    "        else:\n",
    "            raise ValueError(\"First chunk did not return valid JSON\")\n",
    "    \n",
    "    # print(\"JSON inicial creado: \", response.text)\n",
    "    # PASO 2: Refinar con chunks restantes (SIN enviar el schema completo)\n",
    "    for i, chunk in enumerate(legal_doc_chunks[1:], start=2):\n",
    "        print(f\"Procesando chunk {i}/{len(legal_doc_chunks)}...\")\n",
    "        \n",
    "        # Prompt simplificado: instrucciones + JSON actual + nuevo chunk\n",
    "        refine_prompt = (\n",
    "            refine_instructions +\n",
    "            \"\\n\\n**EXISTING JSON:**\\n\" +\n",
    "            json.dumps(refined_json_dict, ensure_ascii=False) +\n",
    "            \"\\n\\n**NEW CHUNK:**\\n\" +\n",
    "            chunk\n",
    "        )\n",
    "        \n",
    "        response = model.generate_content(refine_prompt)\n",
    "        # Actualizar JSON refinado\n",
    "        # refined_json = response.text\n",
    "        \n",
    "        # Parse JSON safely\n",
    "        try:\n",
    "            refined_json_dict = json.loads(response.text)\n",
    "        except json.JSONDecodeError:\n",
    "            match = re.search(r'\\{.*\\}', response.text, re.DOTALL)\n",
    "            if match:\n",
    "                refined_json_dict = json.loads(match.group(0))\n",
    "            else:\n",
    "                print(f\"Warning: Chunk {i} returned invalid JSON, skipping update\")\n",
    "       \n",
    "    # Elastic Search: define index\n",
    "    with open('../elasticSearch/lawai_mapping.json') as f:\n",
    "        lawai_mapping = json.load(f)  \n",
    "    \n",
    "    es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "    index_name = \"lawai_legal_docs\"\n",
    "    \n",
    "    if not es.indices.exists(index=index_name):\n",
    "        es.indices.create(index=index_name, body=lawai_mapping[\"mapping\"])\n",
    "        print(f\"Index '{index_name}' created successfully!\")\n",
    "    else:\n",
    "        print(\"Index already exists.\")\n",
    "    \n",
    "    # Guardar resultado final\n",
    "    print(\"Guardando resultado...\")\n",
    "    with open('./processedDocs/response.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(refined_json_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # refined_json_dict = json.loads(refined_json)\n",
    "    \n",
    "    # generate embeddings for the full text\n",
    "    full_text = refined_json_dict[\"content\"][\"full_text\"]\n",
    "    embedding = get_embedding(full_text)\n",
    "\n",
    "    # 3. store in JSON\n",
    "    refined_json_dict[\"analysis\"][\"embeddings\"] = embedding\n",
    "    \n",
    "    # Index (insert) the document\n",
    "    es.index(\n",
    "        index=index_name, \n",
    "        id=refined_json_dict[\"document_id\"], \n",
    "        document=refined_json_dict\n",
    "    )\n",
    "    print(\"Document indexed successfully!\")\n",
    "    print(\"¡Proceso completado!: \\n\"+ refined_json_dict)\n",
    "\n",
    "    # Query\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"metadata.title\": \"Constitución\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = es.search(index=index_name, body=query)\n",
    "    for hit in results[\"hits\"][\"hits\"]:\n",
    "        print(hit[\"_source\"][\"metadata\"][\"title\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
