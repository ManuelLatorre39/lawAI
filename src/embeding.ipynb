{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea7dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_text(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ascii', 'ignore').decode('utf-8')\n",
    "    #texto = re.sub(r'[^a-z0-9\\s]', '', texto)  # elimina puntuación\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()  # espacios extra\n",
    "    return texto\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f967ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def buscar_patron(texto, etiqueta, patron):\n",
    "    match = re.search(patron, texto, re.IGNORECASE)\n",
    "    if match:\n",
    "        return f\"{etiqueta.upper()}: {match.group(0).strip()}\"\n",
    "    else:\n",
    "        return f\"{etiqueta.upper()}: No disponible\"\n",
    "\n",
    "def extract_metadata(texto):\n",
    "    metadata = {}\n",
    "    metadata['csj'] = buscar_patron(texto, 'csj', r'CSJ\\s*[^\\n\\.]+')\n",
    "    metadata['corte'] = buscar_patron(texto, 'corte', r'CORTE\\s*[^\\n\\.]+')\n",
    "    metadata['provincia'] = buscar_patron(texto, 'provincia', r'BUENOS AIRES\\s*[^\\n\\.]+')\n",
    "    metadata['firma'] = buscar_patron(texto, 'firma', r'FIRMADO\\s*[^\\n\\.]+')\n",
    "    metadata['parte_actora'] = buscar_patron(texto, 'parte_actora', r'PARTE ACTORA\\s*[^\\n\\.]+')\n",
    "    metadata['parte_demandada'] = buscar_patron(texto, 'parte_demandada', r'PARTE DEMANDADA\\s*[^\\n\\.]+')\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "801c7776",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_text\u001b[39m(texto):\n\u001b[32m      4\u001b[39m     splitter = RecursiveCharacterTextSplitter(\n\u001b[32m      5\u001b[39m         chunk_size=\u001b[32m400\u001b[39m,\n\u001b[32m      6\u001b[39m         chunk_overlap=\u001b[32m50\u001b[39m,\n\u001b[32m      7\u001b[39m         separators=[\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m     )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_text(texto):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.split_text(texto)\n",
    "    return [\n",
    "        {\n",
    "            \"original_text\": chunk,\n",
    "            \"cleaned_text\": clean_text(chunk)\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bce004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "modelo = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def generate_embeddings(corpus):\n",
    "    \"\"\"\n",
    "    Genera embeddings para un corpus de texto.\n",
    "    corpus: {\"original_text\": str, \"cleaned_text\": str}\n",
    "    \"\"\"\n",
    "    original_text = [chunk[\"original_text\"] for chunk in corpus]\n",
    "    return modelo.encode(original_text, convert_to_tensor=True, device='cpu')\n",
    "\n",
    "def generate_embeddings_str(string):\n",
    "    \"\"\"\n",
    "    Genera embeddings para un string de texto.\n",
    "    \"\"\"\n",
    "    return modelo.encode(string), \n",
    "\n",
    "\n",
    "def query_similarity(consulta, embeddings_corpus):\n",
    "    \"\"\"\n",
    "    Calcula la similitud entre una consulta y un corpus de texto.\n",
    "    \"\"\"\n",
    "    embedding_consulta = generate_embeddings_str(consulta)\n",
    "    \n",
    "    scores = util.cos_sim(embedding_consulta, embeddings_corpus)\n",
    "    \n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd652323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "def extract_text_from_pdf(pdf_path): \n",
    "    reader = PdfReader(pdf_path)\n",
    "    texto = \"\"\n",
    "    for page in reader.pages:\n",
    "        texto += page.extract_text() + \"\\n\"\n",
    "    return texto.strip()\n",
    "\n",
    "def search_sentence_in_pdf(pdf_path, sentence):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if text and sentence.strip() in text:\n",
    "            return i + 1  # Página donde aparece\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e371edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# main\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = 'falloEj2.pdf'\n",
    "    texto=\"\"\n",
    "    \"\"\" with open('falloEj2.pdf', 'r', encoding='utf-8') as archivo:\n",
    "        texto = archivo.read() \"\"\"\n",
    "    texto = extract_text_from_pdf(pdf_path)\n",
    "    texto_limpio = clean_text(texto)\n",
    "    metadatos = extract_metadata(texto_limpio)\n",
    "    chunks = chunk_text(texto)\n",
    "    corpus = chunks  # usamos los fragmentos como corpus\n",
    "    embeddings_corpus = generate_embeddings(corpus)\n",
    "    consulta = \"notificaciones\"\n",
    "    scores = query_similarity(consulta, embeddings_corpus)\n",
    "    for i, score in enumerate(scores[0]):\n",
    "        print(f\"similitud: {score.item():.2f} (página {search_sentence_in_pdf(pdf_path, corpus[i]['original_text'])}) -> {corpus[i]['cleaned_text']} \")\n",
    "    \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
