{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f039bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os, json\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "597f2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemini_model(model_name=\"gemini-2.0-flash-exp\"):\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"Falta GOOGLE_API_KEY en .env\")\n",
    "    genai.configure(api_key=api_key)\n",
    "    return genai.GenerativeModel(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "675edfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_context(path_schema):\n",
    "    #read schema\n",
    "    with open(path_schema, 'r', encoding='utf-8') as f:\n",
    "        context_data = json.load(f)\n",
    "        context_str = json.dumps(context_data) #convert to string well formatted\n",
    "        context_str = \"**JSON SCHEMA TO FOLLOW:**\\n\" + context_str\n",
    "    return context_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6db5012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(texto):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.split_text(texto)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "42dc3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== CONFIG ======\n",
    "DRY_RUN = False                 # <- ponelo en False cuando estés listo para llamar al LLM\n",
    "MODEL = \"gemini-2.0-flash-exp\" # podés cambiar a uno más barato si querés\n",
    "MAX_CHUNKS = 4                # al principio, procesa poquitos\n",
    "\n",
    "GENERATION_CONFIG = {\n",
    "    \"max_output_tokens\": 2000,  # no te excedas si no hace falta\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "# ====== UTILIDAD: estimar tokens aprox ======\n",
    "# Aproximación grosera: ~4 chars por token (varía por idioma, pero sirve para no volarte)\n",
    "def approx_tokens(text: str) -> int:\n",
    "    return max(1, int(len(text) / 4))\n",
    "\n",
    "# ====== RESPUESTA SIMULADA ======\n",
    "import json, re, time\n",
    "def clean_llm_json(text: str) -> str:\n",
    "    m = re.search(r\"```json(.*?)```\", text, flags=re.DOTALL|re.IGNORECASE)\n",
    "    if m: text = m.group(1).strip()\n",
    "    m = re.search(r\"```(.*?)```\", text, flags=re.DOTALL)\n",
    "    if m: text = m.group(1).strip()\n",
    "    return text.strip()\n",
    "\n",
    "def fake_initial_json(schema_dict, first_chunk):\n",
    "    # simulación: respeta el \"shape\" general y mete un preview\n",
    "    return {\n",
    "        \"_simulated\": True,\n",
    "        \"status\": \"draft\",\n",
    "        \"schema_keys\": list(schema_dict)[:6],\n",
    "        \"notes\": \"Simulación sin LLM. Ajustá prompts y pipeline hasta estar seguro.\",\n",
    "        \"chunk_preview\": first_chunk[:280]\n",
    "    }\n",
    "\n",
    "def fake_refine(existing, new_chunk, i):\n",
    "    # simulación de refinamiento incremental\n",
    "    if isinstance(existing, str):\n",
    "        try:\n",
    "            existing = json.loads(clean_llm_json(existing))\n",
    "        except Exception:\n",
    "            existing = {\"_simulated\": True, \"raw\": existing}\n",
    "    existing[f\"_refined_step_{i}\"] = {\"chunk_preview\": new_chunk[:200]}\n",
    "    return existing\n",
    "\n",
    "# ====== WRAPPER DE ENVÍO ======\n",
    "# Centraliza llamadas: si DRY_RUN, simula; si no, llama al modelo\n",
    "def send_to_llm_initial(model, prompt, schema_dict, first_chunk):\n",
    "    print(f\"[send_to_llm_initial] in≈{approx_tokens(prompt)} tokens\")\n",
    "    if DRY_RUN:\n",
    "        time.sleep(0.3)\n",
    "        return fake_initial_json(schema_dict, first_chunk)\n",
    "    resp = model.generate_content(prompt, generation_config=GENERATION_CONFIG)\n",
    "    return clean_llm_json(resp.text)\n",
    "\n",
    "def send_to_llm_refine(model, prompt, existing_json, new_chunk, step_i):\n",
    "    print(f\"[send_to_llm_refine] in≈{approx_tokens(prompt)} tokens\")\n",
    "    if DRY_RUN:\n",
    "        time.sleep(0.2)\n",
    "        return fake_refine(existing_json, new_chunk, step_i)\n",
    "    resp = model.generate_content(prompt, generation_config=GENERATION_CONFIG)\n",
    "    return clean_llm_json(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "50508f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DRY_RUN=False (REAL). Se llama al modelo.\n",
      "Total chunks: 719\n",
      "Procesando 4 chunks (MAX_CHUNKS=4, DRY_RUN=False)\n",
      "Procesando chunk 1...\n",
      "[send_to_llm_initial] in≈4087 tokens\n",
      "Refinando con chunk 2/4...\n",
      "[send_to_llm_refine] in≈1075 tokens\n",
      "Refinando con chunk 3/4...\n",
      "[send_to_llm_refine] in≈1203 tokens\n",
      "Refinando con chunk 4/4...\n",
      "[send_to_llm_refine] in≈1329 tokens\n",
      "FIN. Guardados:\n",
      " - processedDocs/response.json\n",
      " - processedDocs/response_formatted.json\n"
     ]
    }
   ],
   "source": [
    "# ====== MAIN ROBUSTO: parseo estricto + merge incremental ======\n",
    "import os, json, re\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# -------- Helpers (robustos) --------\n",
    "def clean_llm_json(text: str) -> str:\n",
    "    # quita fences ```json ... ``` o ``` ... ```\n",
    "    m = re.search(r\"```json(.*?)```\", text, flags=re.DOTALL|re.IGNORECASE)\n",
    "    if m: text = m.group(1).strip()\n",
    "    m = re.search(r\"```(.*?)```\", text, flags=re.DOTALL)\n",
    "    if m: text = m.group(1).strip()\n",
    "    return text.strip()\n",
    "\n",
    "def extract_json_block(text: str) -> str:\n",
    "    text = clean_llm_json(text)\n",
    "    for pat in (r\"\\{.*\\}\", r\"\\[.*\\]\"):\n",
    "        m = re.search(pat, text, flags=re.DOTALL)\n",
    "        if m: return m.group(0)\n",
    "    return text.strip()\n",
    "\n",
    "def schema_to_blank_instance(schema):\n",
    "    \"\"\"Construye un objeto vacío con la forma del schema (para no propagar texto crudo).\"\"\"\n",
    "    if not isinstance(schema, dict): \n",
    "        return None\n",
    "    t = schema.get(\"type\")\n",
    "    if t == \"object\":\n",
    "        props = schema.get(\"properties\", {})\n",
    "        return {k: schema_to_blank_instance(v) for k, v in props.items()}\n",
    "    if t == \"array\":    return []\n",
    "    if t == \"string\":   return \"\"\n",
    "    if t in (\"number\",\"integer\",\"boolean\",\"null\"): return None\n",
    "    # fallback si no viene 'type' pero parece objeto\n",
    "    if \"properties\" in schema:\n",
    "        return {k: schema_to_blank_instance(v) for k, v in schema[\"properties\"].items()}\n",
    "    return None\n",
    "\n",
    "def force_json_or_blank(text_or_obj, schema):\n",
    "    \"\"\"Devuelve SIEMPRE dict/list. Si el LLM no da JSON válido, devolvemos esqueleto del schema.\"\"\"\n",
    "    if isinstance(text_or_obj, (dict, list)):\n",
    "        return text_or_obj\n",
    "    s = extract_json_block(str(text_or_obj))\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        return schema_to_blank_instance(schema)\n",
    "\n",
    "def deep_merge(dst, src):\n",
    "    \"\"\"Fusión profunda: integra src en dst sin borrar lo previo.\"\"\"\n",
    "    if not isinstance(dst, dict) or not isinstance(src, dict):\n",
    "        return deepcopy(src)\n",
    "    out = deepcopy(dst)\n",
    "    for k, v in src.items():\n",
    "        if k in out and isinstance(out[k], dict) and isinstance(v, dict):\n",
    "            out[k] = deep_merge(out[k], v)\n",
    "        elif k in out and isinstance(out[k], list) and isinstance(v, list):\n",
    "            # evitar duplicados con hash JSON\n",
    "            seen = set(json.dumps(x, sort_keys=True, ensure_ascii=False) for x in out[k])\n",
    "            out[k] = out[k] + [x for x in v if json.dumps(x, sort_keys=True, ensure_ascii=False) not in seen]\n",
    "        else:\n",
    "            out[k] = deepcopy(v)\n",
    "    return out\n",
    "\n",
    "# -------- Defaults por si no están definidos en otra celda --------\n",
    "if 'DRY_RUN' not in globals():     DRY_RUN = True\n",
    "if 'MAX_CHUNKS' not in globals():  MAX_CHUNKS = 2\n",
    "if 'GENERATION_CONFIG' not in globals():\n",
    "    GENERATION_CONFIG = {\"max_output_tokens\": 2000, \"temperature\": 0.2}\n",
    "\n",
    "# =========================\n",
    "# MAIN (con guardas, parse estricto y merge)\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # 0) Aviso modo\n",
    "    print(\"⚠️ DRY_RUN=True (simulación). No se llama al modelo.\" if DRY_RUN else \"✅ DRY_RUN=False (REAL). Se llama al modelo.\")\n",
    "\n",
    "    # 1) Modelo\n",
    "    model = get_gemini_model(model_name=\"gemini-2.0-flash-exp\")\n",
    "\n",
    "    # 2) Rutas + asserts\n",
    "    PDF_PATH    = Path(\"../../Documentos/CODIGO PENAL DE LA NACION ARGENTINA.pdf\")\n",
    "    SCHEMA_PATH = Path(\"../../JSONS/baseSchema.json\")\n",
    "    assert PDF_PATH.exists(),    f\"No existe el PDF: {PDF_PATH.resolve()}\"\n",
    "    assert SCHEMA_PATH.exists(), f\"No existe el schema: {SCHEMA_PATH.resolve()}\"\n",
    "\n",
    "    # 3) Cargar schema y prompts\n",
    "    with open(SCHEMA_PATH, 'r', encoding='utf-8') as f:\n",
    "        json_schema = json.load(f)\n",
    "    with open('jsonFillPrompt.txt', 'r', encoding='utf-8') as f:\n",
    "        initial_instructions = f.read()\n",
    "    with open('jsonRefinePrompt.txt', 'r', encoding='utf-8') as f:\n",
    "        refine_instructions = f.read()\n",
    "\n",
    "    # 4) PDF -> texto\n",
    "    reader = PdfReader(PDF_PATH.open(\"rb\"))\n",
    "    legal_doc = \"\"\n",
    "    for page in reader.pages:\n",
    "        legal_doc += (page.extract_text() or \"\") + \"\\n\"\n",
    "\n",
    "    # 5) Chunking\n",
    "    legal_doc_chunks = chunk_text(legal_doc)\n",
    "    print(f\"Total chunks: {len(legal_doc_chunks)}\")\n",
    "    legal_doc_chunks = legal_doc_chunks[:MAX_CHUNKS]\n",
    "    print(f\"Procesando {len(legal_doc_chunks)} chunks (MAX_CHUNKS={MAX_CHUNKS}, DRY_RUN={DRY_RUN})\")\n",
    "\n",
    "    # 6) Primer envío (schema completo + primer chunk)\n",
    "    first_chunk_prompt = (\n",
    "        initial_instructions\n",
    "        + \"\\n\\n**JSON SCHEMA:**\\n\"\n",
    "        + json.dumps(json_schema, ensure_ascii=False, indent=2)\n",
    "        + \"\\n\\n**DOCUMENT:**\\n\"\n",
    "        + legal_doc_chunks[0]\n",
    "    )\n",
    "    print(\"Procesando chunk 1...\")\n",
    "    initial_result = send_to_llm_initial(\n",
    "        model=model,\n",
    "        prompt=first_chunk_prompt,\n",
    "        schema_dict=json_schema,\n",
    "        first_chunk=legal_doc_chunks[0]\n",
    "    )\n",
    "    # Clave: no propagar texto crudo\n",
    "    result = force_json_or_blank(initial_result, json_schema)\n",
    "\n",
    "    # 7) Refinamiento incremental con MERGE (no pisar lo previo)\n",
    "    for i, chunk in enumerate(legal_doc_chunks[1:], start=2):\n",
    "        refine_prompt = (\n",
    "            refine_instructions\n",
    "            + \"\\n\\n**EXISTING JSON:**\\n\"\n",
    "            + json.dumps(result, ensure_ascii=False, indent=2)\n",
    "            + \"\\n\\n**NEW CHUNK:**\\n\"\n",
    "            + chunk\n",
    "        )\n",
    "        print(f\"Refinando con chunk {i}/{len(legal_doc_chunks)}...\")\n",
    "        refined = send_to_llm_refine(\n",
    "            model=model,\n",
    "            prompt=refine_prompt,\n",
    "            existing_json=result,\n",
    "            new_chunk=chunk,\n",
    "            step_i=i\n",
    "        )\n",
    "        refined = force_json_or_blank(refined, json_schema)\n",
    "        result  = deep_merge(result, refined)\n",
    "\n",
    "    # 8) Guardado consistente\n",
    "    out_dir = Path(\"./processedDocs\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with (out_dir / \"response.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Variante legible (solo convierte '\\n' a saltos reales en strings)\n",
    "    def _readable(o):\n",
    "        if isinstance(o, dict):  return {k: _readable(v) for k, v in o.items()}\n",
    "        if isinstance(o, list):  return [_readable(x) for x in o]\n",
    "        if isinstance(o, str):   return o.replace(\"\\\\n\", \"\\n\")\n",
    "        return o\n",
    "    with (out_dir / \"response_formatted.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(_readable(result), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"FIN. Guardados:\\n - processedDocs/response.json\\n - processedDocs/response_formatted.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
